{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3bc324b",
   "metadata": {},
   "source": [
    "<h1>Optimisation Assignment aka \"build it from scratch\"</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6685140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "import random\n",
    "import pandas as pd\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1994a0a",
   "metadata": {},
   "source": [
    "<h2>Question 1: Write a code to find the root of the function, that is, find x where the value of the following function is 0.</h2>\n",
    "Take the equation ùêπ(ùë•)=ùë•‚àísin(ùë•).\n",
    "\n",
    "Note that ùêπ‚Ä≤(ùë•)=1‚àícos(ùë•). Apply Newton's method with ùë•0=1 (initial point) and $\\epsilon=10^{-5}$ (error should be smaller than this, that is F(x) < $\\epsilon$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e7860dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x - np.sin(x), 1-np.cos(x)\n",
    "\n",
    "def newtonEquation(x0, eps, maxiters = 100): \n",
    "    k = 0\n",
    "    x = x0\n",
    "    f, g = func(x)\n",
    "    while np.abs(f) > eps and k <= maxiters:\n",
    "        print(f\"Iteration {k}: x = {round(x,4)} f(x) = {round(f,5)} g = {round(g, 5)}\")\n",
    "        k += 1\n",
    "        if g == 0.0:\n",
    "            g = g+0.01\n",
    "            return \"Solution reached as f' is close to 0\"\n",
    "        try:    \n",
    "            x = x - f / g\n",
    "        except:\n",
    "            message = f'problems at iteration {k}'\n",
    "            return x, message\n",
    "        f, g = func(x)\n",
    "    if np.abs(f) <= eps:\n",
    "        return round(x, 4), f'Precision has been reached: {round(np.abs(f),6)} <= {eps}'\n",
    "    else:\n",
    "        return None, f'Maximum number of iterations reached: {maxiters}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d7d9ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = 1 f(x) = 0.15853 g = 0.4597\n",
      "Iteration 1: x = 0.6551 f(x) = 0.04587 g = 0.20704\n",
      "Iteration 2: x = 0.4336 f(x) = 0.01346 g = 0.09254\n",
      "Iteration 3: x = 0.2881 f(x) = 0.00397 g = 0.04123\n",
      "Iteration 4: x = 0.1918 f(x) = 0.00117 g = 0.01834\n",
      "Iteration 5: x = 0.1278 f(x) = 0.00035 g = 0.00816\n",
      "Iteration 6: x = 0.0852 f(x) = 0.0001 g = 0.00363\n",
      "Iteration 7: x = 0.0568 f(x) = 3e-05 g = 0.00161\n",
      "Solution: 0.0379 \n",
      "Reason: Precision has been reached: 9e-06 <= 1e-05\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "x0  = 1\n",
    "eps = 1.0e-5\n",
    "x, message = newtonEquation(x0, eps)\n",
    "\n",
    "print(f'Solution: {x} \\nReason: {message}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c383d2c",
   "metadata": {},
   "source": [
    "<h1>Multi-Variable Optimization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22552a7",
   "metadata": {},
   "source": [
    "Consider the function \\\\[f(x_1,x_2) = 2x_1^3+6x_1x_2^2 -3x_2^3-150x_1. \\\\]\n",
    "\n",
    "<h2>Question 2. Write a code to find the optimal point (optimal solutions) for the 2-variable function. The gradient and the hessian is shown below.</h2>\n",
    "\n",
    "<b>Note</b> that when calculating -g/H in python, you can use a function such that direction = np.linalg.solve(H, -g) \n",
    "\n",
    "Then the gradient is \\\\[ \\nabla f(x) = \\left( \\begin{array}{c} 6x_1^2+6x_2^2-150 \\\\ 12x_1x_2-9x_2^2 \\end{array} \\right), \\\\]\n",
    "and the Hessian is  \\\\[\\nabla^2 f(x) = \\left( \\begin{array}{cc} 12x_1 & 12x_2\\\\12x_2 & 12x_1-18x_2\\end{array} \\right). \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd72fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return 2*x1**3 + 6*x1*x2**2 - 3*x2**3 - 150*x1\n",
    "def df2(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return np.array([6*x1**2 + 6*x2**2 -150, 12*x1*x2 - 9*x2**2])\n",
    "def ddf2(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    return np.array([[12*x1, 12*x2],[12*x2, 12*x1 - 18*x2]])\n",
    "\n",
    "def newtonEquation_2(x0, eps, maxiters = 100): \n",
    "\n",
    "    k = 0\n",
    "    x = x0\n",
    "    f = f2(x)\n",
    "    g = df2(x)\n",
    "    h = ddf2(x)\n",
    "    \n",
    "    while np.abs(g.all()) >= eps and k <= maxiters:\n",
    "        print(np.round(g,4))\n",
    "        print(f'derivative value = {f2(x)}')\n",
    "        k += 1\n",
    "        if g.all() == 0.0:\n",
    "            g = g + 0.01\n",
    "            return \"Solution reached as f'' is close to 0 \"\n",
    "        try:    \n",
    "            x = x + np.linalg.solve(h, -g)\n",
    "        except:\n",
    "            message = f'problems on iteration {k}'\n",
    "            return x, message\n",
    "        g = df2(x)\n",
    "        h = ddf2(x)\n",
    "    if np.abs(g.all()) <= eps:\n",
    "        return x, f'Precision has been reached: {np.abs(g[0])} <= {eps}'\n",
    "    else:\n",
    "        return None, f'Maximum number of iterations reached: {maxiters}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c99df460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-138    3]\n",
      "derivative value = -145\n",
      "[ 448.8333 -207.5833]\n",
      "derivative value = -379.69907407407425\n",
      "[ 84.9079 -49.5697]\n",
      "derivative value = -320.8667876349568\n",
      "[ 8.5108 -8.42  ]\n",
      "derivative value = -300.89376961264463\n",
      "[ 0.2072 -0.3545]\n",
      "derivative value = -300.00139036701705\n",
      "[ 0.0003 -0.0006]\n",
      "derivative value = -300.00000000322336\n",
      "[ 0. -0.]\n",
      "derivative value = -300.00000000000006\n",
      "Solution: x1 = 3.0, x2 = 4.0 \n",
      "Diagnostic: Precision has been reached: 0.0 <= 1e-05\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "x0  = [1, 1]\n",
    "eps = 1.0e-5\n",
    "x, message = newtonEquation_2(x0, eps)\n",
    "\n",
    "print(f'Solution: x1 = {round(x[0],4)}, x2 = {round(x[1],4)} \\nDiagnostic: {message}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c507d",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression Using Stochastic Gradient Descent</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189b951",
   "metadata": {},
   "source": [
    "In a logistic regression model, the prediction is given by\n",
    "\n",
    "$p(x_i) = \\frac{1}{1+exp(-f(x))}$ where $f(x)$ is given by:\n",
    "\n",
    "$f(x) = \\theta_0+\\theta_1x_1+\\theta_2x_2...$ <br>\n",
    "where $f(x)$ is a linear function (hence logistic regression is called generalized linear regression)\n",
    "\n",
    "We minimize the negative of maximum likelihood function. Thus the loss function we want to minimize is:<br>\n",
    "$minimize: loss(\\theta) = -\\frac{1}{n}\\sum_i\\bigg( \\big(y_i (ln(p(x_i))\\big)+(1-y_i)(1-ln(p(x_i))\\bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a523556",
   "metadata": {},
   "source": [
    "The gradient of the cost function (in direction $j$ is given by (where m is a batch size, you can chose any batch sixe): <br>\n",
    "$\\frac{d(loss(\\theta_))}{d\\theta} = \\frac{1}{m}\\sum_i \\bigg(y_i-p(x_i)\\bigg)x_i^j$\n",
    "\n",
    "Therefore, steps in direction $j$ is given by:<br>\n",
    "$\\theta_j (n+1) = \\theta_j (n)-\\alpha \\frac{1}{m}\\sum_i \\bigg(y_i-p(x_i)\\bigg)x_i^j$\n",
    "\n",
    "Try deriving this from the equation above. Hint, keep it as $p(x_i)$ form, easier for derivation.\n",
    "\n",
    "Check this link to study how the gradients are calculated:\n",
    "https://www.baeldung.com/cs/gradient-descent-logistic-regression\n",
    "\n",
    "<h2>Question 3: Write a code to find the coefficients for $\\theta_1$ and $\\theta_2$ (use at least 10 iterations and find the coefficients after 10 iterations)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141edf06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 2 columns are the predictor variables and 3rd column is the output variable (0 or 1)\n",
    "# marks in 2 exams and if they got admission or not\n",
    "# in most machine learning problems, it is better to normalize the data for the training set.\n",
    "\n",
    "data = np.array([\n",
    "[34.62365962451697,78.0246928153624,0],\n",
    "[30.28671076822607,43.89499752400101,0],\n",
    "[35.84740876993872,72.90219802708364,0],\n",
    "[60.18259938620976,86.30855209546826,1],\n",
    "[79.0327360507101,75.3443764369103,1],\n",
    "[45.08327747668339,56.3163717815305,0],\n",
    "[61.10666453684766,96.51142588489624,1],\n",
    "[75.02474556738889,46.55401354116538,1],\n",
    "[76.09878670226257,87.42056971926803,1],\n",
    "[84.43281996120035,43.53339331072109,1],\n",
    "[95.86155507093572,38.22527805795094,0],\n",
    "[75.01365838958247,30.60326323428011,0],\n",
    "[82.30705337399482,76.48196330235604,1],\n",
    "[69.36458875970939,97.71869196188608,1],\n",
    "[39.53833914367223,76.03681085115882,0],\n",
    "[53.9710521485623,89.20735013750205,1],\n",
    "[69.07014406283025,52.74046973016765,1],\n",
    "[67.94685547711617,46.67857410673128,0],\n",
    "[70.66150955499435,92.92713789364831,1],\n",
    "[76.97878372747498,47.57596364975532,1],\n",
    "[67.37202754570876,42.83843832029179,0],\n",
    "[89.67677575072079,65.79936592745237,1],\n",
    "[50.534788289883,48.85581152764205,0],\n",
    "[34.21206097786789,44.20952859866288,0],\n",
    "[77.9240914545704,68.9723599933059,1],\n",
    "[62.27101367004632,69.95445795447587,1],\n",
    "[80.1901807509566,44.82162893218353,1],\n",
    "[93.114388797442,38.80067033713209,0],\n",
    "[61.83020602312595,50.25610789244621,0],\n",
    "[38.78580379679423,64.99568095539578,0],\n",
    "[61.379289447425,72.80788731317097,1],\n",
    "[85.40451939411645,57.05198397627122,1],\n",
    "[52.10797973193984,63.12762376881715,0],\n",
    "[52.04540476831827,69.43286012045222,1],\n",
    "[40.23689373545111,71.16774802184875,0],\n",
    "[54.63510555424817,52.21388588061123,0],\n",
    "[33.91550010906887,98.86943574220611,0],\n",
    "[64.17698887494485,80.90806058670817,1],\n",
    "[74.78925295941542,41.57341522824434,0],\n",
    "[34.1836400264419,75.2377203360134,0],\n",
    "[83.90239366249155,56.30804621605327,1],\n",
    "[51.54772026906181,46.85629026349976,0],\n",
    "[94.44336776917852,65.56892160559052,1],\n",
    "[82.36875375713919,40.61825515970618,0],\n",
    "[51.04775177128865,45.82270145776001,0],\n",
    "[62.22267576120188,52.06099194836679,0],\n",
    "[77.19303492601364,70.45820000180959,1],\n",
    "[97.77159928000232,86.7278223300282,1],\n",
    "[62.07306379667647,96.76882412413983,1],\n",
    "[91.56497449807442,88.69629254546599,1],\n",
    "[79.94481794066932,74.16311935043758,1],\n",
    "[99.2725269292572,60.99903099844988,1],\n",
    "[90.54671411399852,43.39060180650027,1],\n",
    "[34.52451385320009,60.39634245837173,0],\n",
    "[50.2864961189907,49.80453881323059,0],\n",
    "[49.58667721632031,59.80895099453265,0],\n",
    "[97.64563396007767,68.86157272420604,1],\n",
    "[32.57720016809309,95.59854761387875,0],\n",
    "[74.24869136721598,69.82457122657193,1],\n",
    "[71.79646205863379,78.45356224515052,1],\n",
    "[75.3956114656803,85.75993667331619,1],\n",
    "[35.28611281526193,47.02051394723416,0],\n",
    "[56.25381749711624,39.26147251058019,0],\n",
    "[30.05882244669796,49.59297386723685,0],\n",
    "[44.66826172480893,66.45008614558913,0],\n",
    "[66.56089447242954,41.09209807936973,0],\n",
    "[40.45755098375164,97.53518548909936,1],\n",
    "[49.07256321908844,51.88321182073966,0],\n",
    "[80.27957401466998,92.11606081344084,1],\n",
    "[66.74671856944039,60.99139402740988,1],\n",
    "[32.72283304060323,43.30717306430063,0],\n",
    "[64.0393204150601,78.03168802018232,1],\n",
    "[72.34649422579923,96.22759296761404,1],\n",
    "[60.45788573918959,73.09499809758037,1],\n",
    "[58.84095621726802,75.85844831279042,1],\n",
    "[99.82785779692128,72.36925193383885,1],\n",
    "[47.26426910848174,88.47586499559782,1],\n",
    "[50.45815980285988,75.80985952982456,1],\n",
    "[60.45555629271532,42.50840943572217,0],\n",
    "[82.22666157785568,42.71987853716458,0],\n",
    "[88.9138964166533,69.80378889835472,1],\n",
    "[94.83450672430196,45.69430680250754,1],\n",
    "[67.31925746917527,66.58935317747915,1],\n",
    "[57.23870631569862,59.51428198012956,1],\n",
    "[80.36675600171273,90.96014789746954,1],\n",
    "[68.46852178591112,85.59430710452014,1],\n",
    "[42.0754545384731,78.84478600148043,0],\n",
    "[75.47770200533905,90.42453899753964,1],\n",
    "[78.63542434898018,96.64742716885644,1],\n",
    "[52.34800398794107,60.76950525602592,0],\n",
    "[94.09433112516793,77.15910509073893,1],\n",
    "[90.44855097096364,87.50879176484702,1],\n",
    "[55.48216114069585,35.57070347228866,0],\n",
    "[74.49269241843041,84.84513684930135,1],\n",
    "[89.84580670720979,45.35828361091658,1],\n",
    "[83.48916274498238,48.38028579728175,1],\n",
    "[42.2617008099817,87.10385094025457,1],\n",
    "[99.31500880510394,68.77540947206617,1],\n",
    "[55.34001756003703,64.9319380069486,1],\n",
    "[74.77589300092767,89.52981289513276,1],\n",
    "])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6bee7785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yhat(row, coefs):\n",
    "    yhat = coefs[0]\n",
    "    for i in range(len(row)-1):\n",
    "        yhat += coefs[i + 1] * row[i]\n",
    "    return 1/ (1 + exp(-yhat))\n",
    "\n",
    "def sgd(data, learn_rate, max_iter, eps):\n",
    "    coef = [0 for i in range(len(data[0]))]\n",
    "    k = 0\n",
    "    sse_list = []\n",
    "    sse_improv = 1\n",
    "    while k <= max_iter and np.abs(sse_improv) >= eps:\n",
    "        sse = 0\n",
    "        for row in data:\n",
    "            yhat = get_yhat(row, coef)\n",
    "            e = row[-1] - yhat\n",
    "            sse += e**2\n",
    "            coef[0] = coef[0] + learn_rate * e * yhat * (1 - yhat)\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] + learn_rate * e * yhat * (1 - yhat) * row[i]\n",
    "        sse_list.append(sse)\n",
    "        if k>0:\n",
    "            sse_improv = sse_list[k] - sse_list[k-1]\n",
    "        print(f'interation = {k},  error = {round(sse,4)}, error_improvement = {np.abs(round(sse_improv, 4))}')\n",
    "        k += 1\n",
    "    return coef\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1bde1b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation = 0,  error = 60.2382, error_improvement = 1\n",
      "interation = 1,  error = 59.9745, error_improvement = 0.2637\n",
      "interation = 2,  error = 51.101, error_improvement = 8.8735\n",
      "interation = 3,  error = 39.9979, error_improvement = 11.1032\n",
      "interation = 4,  error = 39.9978, error_improvement = 0.0001\n",
      "interation = 5,  error = 39.9977, error_improvement = 0.0001\n",
      "interation = 6,  error = 39.9976, error_improvement = 0.0001\n",
      "interation = 7,  error = 39.9975, error_improvement = 0.0001\n",
      "interation = 8,  error = 39.9973, error_improvement = 0.0001\n",
      "interation = 9,  error = 39.9972, error_improvement = 0.0001\n",
      "interation = 10,  error = 39.997, error_improvement = 0.0002\n",
      "interation = 11,  error = 39.9969, error_improvement = 0.0002\n",
      "interation = 12,  error = 39.9967, error_improvement = 0.0002\n",
      "interation = 13,  error = 39.9964, error_improvement = 0.0002\n",
      "interation = 14,  error = 39.9962, error_improvement = 0.0003\n",
      "interation = 15,  error = 39.9959, error_improvement = 0.0003\n",
      "interation = 16,  error = 39.9955, error_improvement = 0.0004\n",
      "interation = 17,  error = 39.9951, error_improvement = 0.0004\n",
      "interation = 18,  error = 39.9945, error_improvement = 0.0005\n",
      "interation = 19,  error = 39.9938, error_improvement = 0.0007\n",
      "interation = 20,  error = 39.9929, error_improvement = 0.0009\n",
      "interation = 21,  error = 39.9917, error_improvement = 0.0012\n",
      "interation = 22,  error = 39.99, error_improvement = 0.0017\n",
      "interation = 23,  error = 39.9872, error_improvement = 0.0028\n",
      "interation = 24,  error = 39.9822, error_improvement = 0.0051\n",
      "interation = 25,  error = 39.9694, error_improvement = 0.0128\n",
      "interation = 26,  error = 39.8512, error_improvement = 0.1182\n",
      "interation = 27,  error = 39.9466, error_improvement = 0.0954\n",
      "interation = 28,  error = 58.8708, error_improvement = 18.9242\n",
      "interation = 29,  error = 59.9834, error_improvement = 1.1126\n",
      "interation = 30,  error = 59.9329, error_improvement = 0.0505\n",
      "interation = 31,  error = 60.5658, error_improvement = 0.6329\n",
      "interation = 32,  error = 60.409, error_improvement = 0.1568\n",
      "interation = 33,  error = 60.0, error_improvement = 0.409\n",
      "interation = 34,  error = 60.0, error_improvement = 0.0\n",
      "teta1 = -0.0294, teta2 = -0.3194\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "learn_rate = 0.01\n",
    "max_iter = 100\n",
    "eps = 1.0e-5\n",
    "coefs = sgd(data, learn_rate, max_iter, eps)\n",
    "teta_1 = coefs[1]\n",
    "teta_2 = coefs[2]\n",
    "print(f'teta1 = {round(teta_1,4)}, teta2 = {round(teta_2,4)}')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
